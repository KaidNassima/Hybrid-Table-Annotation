{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95521084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b1aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"path to Artifacts/Experiments and Results/CPA_all.pkl\", \"rb\") as f:\n",
    "    predictions = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540660d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(\"path to Benchmark\\\\CPA_Test\\\\CPA_test_gt.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground['label_gt'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e342802",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des relations d'équivalence et subPropertyOf\n",
    "import pandas as pd\n",
    "\n",
    "mapping_df = pd.read_excel(\"path to Artifacts/Experiments and Results/cpa_to_schema_org_mapping.xlsx\")  \n",
    "\n",
    "def normalize(prop):\n",
    "    \"\"\"Extrait la dernière partie d'une URI ou garde tel quel, en minuscules\"\"\"\n",
    "    if isinstance(prop, str):\n",
    "        return prop.strip().split(\"/\")[-1].lower()\n",
    "    return \"\"\n",
    "\n",
    "equivalents = {}\n",
    "sub_properties = {}\n",
    "\n",
    "for _, row in mapping_df.iterrows():\n",
    "    cpa_label = normalize(row[\"cpa_label\"])\n",
    "    \n",
    "    equiv = normalize(row.get(\"equivalentProperty\", \"\"))\n",
    "    if equiv:\n",
    "        equivalents.setdefault(cpa_label, set()).add(equiv)\n",
    "        equivalents.setdefault(equiv, set()).add(cpa_label)  # symétrique\n",
    "\n",
    "    sub = normalize(row.get(\"subPropertyOf\", \"\"))\n",
    "    if sub:\n",
    "        sub_properties.setdefault(cpa_label, set()).add(sub)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detailed_results = []\n",
    "\n",
    "true_positives = 0\n",
    "total_gt = 0\n",
    "total_predicted = 0\n",
    "top1_hits = 0\n",
    "top3_hits = 0\n",
    "\n",
    "for _, row in ground_truth.iterrows():\n",
    "    table_name = row[\"table_name\"]\n",
    "    main_col = int(row[\"main_column_index\"])\n",
    "    col = int(row[\"column_index\"])\n",
    "    label_gt = row[\"label\"].strip().lower()\n",
    "\n",
    "    table_annotations = predictions.get(table_name, {})\n",
    "    relation_dict = table_annotations.get((main_col, col), {})\n",
    "\n",
    "    predicted_relations = list(relation_dict.keys())\n",
    "    predicted_relations_lower = [r.lower() for r in predicted_relations]\n",
    "\n",
    "    # tri des relations par proba \n",
    "    sorted_rels = sorted(\n",
    "        relation_dict.items(),\n",
    "        key=lambda x: x[1].get(\"probability\", 0),\n",
    "        reverse=True\n",
    "    )\n",
    "    sorted_relation_names = [r[0].lower() for r in sorted_rels]\n",
    "\n",
    "    # match = label_gt in predicted_relations_lower\n",
    "    match = False\n",
    "    equiv_set = equivalents.get(label_gt, set())\n",
    "    sub_set = sub_properties.get(label_gt, set())\n",
    "\n",
    "    for pred in predicted_relations_lower:\n",
    "        if pred == label_gt:\n",
    "            match = True\n",
    "            break\n",
    "        if pred in equiv_set:\n",
    "            match = True\n",
    "            break\n",
    "        if label_gt in sub_properties.get(pred, set()):  # predicted ⊆ ground truth\n",
    "            match = True\n",
    "            break\n",
    "\n",
    "    top1 = label_gt == sorted_relation_names[0] if sorted_relation_names else False\n",
    "    top3 = label_gt in sorted_relation_names[:3]\n",
    "\n",
    "    detailed_results.append({\n",
    "        \"table_name\": table_name,\n",
    "        \"main_col\": main_col,\n",
    "        \"col\": col,\n",
    "        \"label_gt\": label_gt,\n",
    "        \"predicted_relations\": predicted_relations,\n",
    "        \"match\": match,\n",
    "        \"top1_match\": top1,\n",
    "        \"top3_match\": top3,\n",
    "    })\n",
    "\n",
    "    total_gt += 1\n",
    "    total_predicted += len(predicted_relations)\n",
    "    if match:\n",
    "        true_positives += 1\n",
    "    if top1:\n",
    "        top1_hits += 1\n",
    "    if top3:\n",
    "        top3_hits += 1\n",
    "\n",
    "# --- Scores globaux ---\n",
    "recall = true_positives / total_gt if total_gt else 0\n",
    "precision = true_positives / total_predicted if total_predicted else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
    "top1_acc = top1_hits / total_gt\n",
    "top3_acc = top3_hits / total_gt\n",
    "\n",
    "# --- Affichage ---\n",
    "print(\"== Résultats globaux ==\")\n",
    "print(\"true_positives :\", true_positives)\n",
    "print(\"total_gt :\", total_gt)\n",
    "print(\"total_predicted :\", total_predicted)\n",
    "print(f\"Recall : {recall:.3f}\")\n",
    "print(f\"Precision : {precision:.3f}\")\n",
    "print(f\"F1-score : {f1:.3f}\")\n",
    "print(f\"Top-1 Accuracy : {top1_acc:.3f}\")\n",
    "print(f\"Top-3 Accuracy : {top3_acc:.3f}\")\n",
    "\n",
    "# # --- Sauvegarde optionnelle ---\n",
    "# df_results = pd.DataFrame(detailed_results)\n",
    "# df_results.to_csv(\"evaluation_détaillée_relations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e673b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scores Top-1 comparables à Doduo ---\n",
    "tp_top1 = 0\n",
    "fp_top1 = 0\n",
    "fn_top1 = 0\n",
    "\n",
    "for res in detailed_results:\n",
    "    gt = res[\"label_gt\"]\n",
    "    sorted_preds = [r.lower() for r in res[\"predicted_relations\"]]  \n",
    "\n",
    "    if not sorted_preds:  \n",
    "        # Pas de prédiction = faux négatif\n",
    "        fn_top1 += 1\n",
    "        continue\n",
    "\n",
    "    top1_pred = sorted_preds[0]  # la relation avec la proba max\n",
    "\n",
    "    if gt == top1_pred:\n",
    "        tp_top1 += 1\n",
    "    else:\n",
    "        fp_top1 += 1\n",
    "        fn_top1 += 1\n",
    "\n",
    "precision_top1 = tp_top1 / (tp_top1 + fp_top1) if (tp_top1 + fp_top1) else 0\n",
    "recall_top1 = tp_top1 / (tp_top1 + fn_top1) if (tp_top1 + fn_top1) else 0\n",
    "f1_top1 = 2 * precision_top1 * recall_top1 / (precision_top1 + recall_top1) if (precision_top1 + recall_top1) else 0\n",
    "\n",
    "print(\"\\n== Résultats comparables à Doduo (Top-1) ==\")\n",
    "print(\"TP :\", tp_top1)\n",
    "print(\"FP :\", fp_top1)\n",
    "print(\"FN :\", fn_top1)\n",
    "print(f\"Precision Top-1 : {precision_top1:.3f}\")\n",
    "print(f\"Recall Top-1    : {recall_top1:.3f}\")\n",
    "print(f\"F1-score Top-1  : {f1_top1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30100be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "detailed_results = []\n",
    "true_positives = {\"LM\": 0, \"KB\": 0}\n",
    "total_gt = 0\n",
    "total_predicted = {\"LM\": 0, \"KB\": 0}\n",
    "top1_hits = {\"LM\": 0, \"KB\": 0}\n",
    "top3_hits = {\"LM\": 0, \"KB\": 0}\n",
    "\n",
    "for _, row in ground_truth.iterrows():\n",
    "    table_name = row[\"table_name\"]\n",
    "    main_col = int(row[\"main_column_index\"])\n",
    "    col = int(row[\"column_index\"])\n",
    "    label_gt = row[\"label\"].strip().lower()\n",
    "\n",
    "    table_annotations = predictions.get(table_name, {})\n",
    "    relation_dict = table_annotations.get((main_col, col), {})\n",
    "\n",
    "    # --- Split relations by source ---\n",
    "    rels_LM = {r: info for r, info in relation_dict.items() if \"LM\" in info.get(\"sources\", [])}\n",
    "    rels_KB = {r: info for r, info in relation_dict.items() if any(\"KB\" in s for s in info.get(\"sources\", []))}\n",
    "\n",
    "    # Utility: function for evaluating a single source set\n",
    "    def evaluate_source(source_name, rels):\n",
    "        predicted_relations = list(rels.keys())\n",
    "        predicted_relations_lower = [r.lower() for r in predicted_relations]\n",
    "\n",
    "        sorted_rels = sorted(\n",
    "            rels.items(),\n",
    "            key=lambda x: x[1].get(\"probability\", 0),\n",
    "            reverse=True\n",
    "        )\n",
    "        sorted_relation_names = [r[0].lower() for r in sorted_rels]\n",
    "\n",
    "        match = False\n",
    "        equiv_set = equivalents.get(label_gt, set())\n",
    "        sub_set = sub_properties.get(label_gt, set())\n",
    "\n",
    "        for pred in predicted_relations_lower:\n",
    "            if pred == label_gt:\n",
    "                match = True\n",
    "                break\n",
    "            if pred in equiv_set:\n",
    "                match = True\n",
    "                break\n",
    "            if label_gt in sub_properties.get(pred, set()):  # predicted ⊆ ground truth\n",
    "                match = True\n",
    "                break\n",
    "\n",
    "        top1 = label_gt == sorted_relation_names[0] if sorted_relation_names else False\n",
    "        top3 = label_gt in sorted_relation_names[:3]\n",
    "\n",
    "        total_predicted[source_name] += len(predicted_relations)\n",
    "        if match:\n",
    "            true_positives[source_name] += 1\n",
    "        if top1:\n",
    "            top1_hits[source_name] += 1\n",
    "        if top3:\n",
    "            top3_hits[source_name] += 1\n",
    "\n",
    "        detailed_results.append({\n",
    "            \"table_name\": table_name,\n",
    "            \"main_col\": main_col,\n",
    "            \"col\": col,\n",
    "            \"label_gt\": label_gt,\n",
    "            \"predicted_relations\": predicted_relations,\n",
    "            \"source\": source_name,\n",
    "            \"match\": match,\n",
    "            \"top1_match\": top1,\n",
    "            \"top3_match\": top3,\n",
    "        })\n",
    "\n",
    "    # Evaluate separately for LM and KB\n",
    "    evaluate_source(\"LM\", rels_LM)\n",
    "    evaluate_source(\"KB\", rels_KB)\n",
    "\n",
    "    total_gt += 1\n",
    "\n",
    "# --- Compute metrics per source ---\n",
    "for src in [\"LM\", \"KB\"]:\n",
    "    recall = true_positives[src] / total_gt if total_gt else 0\n",
    "    precision = true_positives[src] / total_predicted[src] if total_predicted[src] else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
    "    top1_acc = top1_hits[src] / total_gt\n",
    "    top3_acc = top3_hits[src] / total_gt\n",
    "\n",
    "    print(f\"== Résultats {src} ==\")\n",
    "    print(f\"True positives : {true_positives[src]}\")\n",
    "    print(f\"Total GT : {total_gt}\")\n",
    "    print(f\"Total predicted : {total_predicted[src]}\")\n",
    "    print(f\"Recall : {recall:.3f}\")\n",
    "    print(f\"Precision : {precision:.3f}\")\n",
    "    print(f\"F1-score : {f1:.3f}\")\n",
    "    print(f\"Top-1 Accuracy : {top1_acc:.3f}\")\n",
    "    print(f\"Top-3 Accuracy : {top3_acc:.3f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# kb_predictions_per_pair : pour stocker ce que KB prédit par paire de colonnes\n",
    "kb_predictions_per_pair = defaultdict(list)\n",
    "\n",
    "for table_name, table_annotations in predictions.items():\n",
    "    for (main_col, col), relation_dict in table_annotations.items():\n",
    "        # On ne prend que les relations provenant de KB\n",
    "        kb_rels = [r for r, info in relation_dict.items() if any(\"KB\" in s for s in info.get(\"sources\", []))]\n",
    "        if kb_rels:  # si la KB a produit quelque chose\n",
    "            # Ground truth pour cette paire\n",
    "            gt_row = ground_truth[(ground_truth[\"table_name\"] == table_name) &\n",
    "                                  (ground_truth[\"main_column_index\"] == main_col) &\n",
    "                                  (ground_truth[\"column_index\"] == col)]\n",
    "            gt_rel = gt_row[\"label\"].values[0] if not gt_row.empty else None\n",
    "\n",
    "            kb_predictions_per_pair[(table_name, main_col, col)].append({\n",
    "                \"ground_truth\": gt_rel,\n",
    "                \"kb_predictions\": kb_rels\n",
    "            })\n",
    "\n",
    "# --- Affichage ---\n",
    "for (table_name, main_col, col), infos in kb_predictions_per_pair.items():\n",
    "    for info in infos:\n",
    "        print(f\"Table: {table_name}, Main_col: {main_col}, Col: {col}\")\n",
    "        print(f\"  Ground truth: {info['ground_truth']}\")\n",
    "        print(f\"  KB predictions: {info['kb_predictions']}\")\n",
    "        print(\"----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9435eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "total_pairs_kb = 0\n",
    "correct_pairs = 0\n",
    "pairs_outside_gt = 0\n",
    "extra_when_correct = []\n",
    "\n",
    "kb_extra_per_table = defaultdict(list)\n",
    "kb_predictions_outside_gt = Counter()\n",
    "\n",
    "for table_name, table_annotations in predictions.items():\n",
    "    for (main_col, col), relation_dict in table_annotations.items():\n",
    "        # Prendre uniquement les relations KB\n",
    "        kb_rels = [r for r, info in relation_dict.items() if any(\"KB\" in s for s in info.get(\"sources\", []))]\n",
    "        if kb_rels:\n",
    "            total_pairs_kb += 1\n",
    "\n",
    "            # Ground truth pour cette paire\n",
    "            gt_row = ground_truth[(ground_truth[\"table_name\"] == table_name) &\n",
    "                                  (ground_truth[\"main_column_index\"] == main_col) &\n",
    "                                  (ground_truth[\"column_index\"] == col)]\n",
    "            gt_rel = gt_row[\"label\"].values[0] if not gt_row.empty else None\n",
    "\n",
    "            # Vérifier si KB a prédit correctement\n",
    "            if gt_rel and gt_rel.lower() in [r.lower() for r in kb_rels]:\n",
    "                correct_pairs += 1\n",
    "                # compter combien de relations supplémentaires hors GT\n",
    "                extra_count = len([r for r in kb_rels if r.lower() != gt_rel.lower()])\n",
    "                extra_when_correct.append(extra_count)\n",
    "            else:\n",
    "                pairs_outside_gt += 1\n",
    "                kb_predictions_outside_gt.update(kb_rels)\n",
    "                kb_extra_per_table[table_name].append((main_col, col, kb_rels))\n",
    "\n",
    "# --- Affichage des quantifications ---\n",
    "print(f\"Total paires annotées par KB : {total_pairs_kb}\")\n",
    "print(f\"Paires correctes (match GT) : {correct_pairs}\")\n",
    "print(f\"Paires hors ground truth : {pairs_outside_gt}\")\n",
    "print(f\"Précision locale KB : {correct_pairs / total_pairs_kb * 100:.2f}%\")\n",
    "\n",
    "mean_extra_per_table = sum(len(r) for r in kb_extra_per_table.values()) / len(kb_extra_per_table)\n",
    "print(f\"Nombre moyen de relations hors GT par table : {mean_extra_per_table:.2f}\")\n",
    "\n",
    "# Moyenne de relations supplémentaires par paire correcte\n",
    "mean_extra_correct = sum(extra_when_correct) / len(extra_when_correct) if extra_when_correct else 0\n",
    "print(f\"Moyenne de relations supplémentaires KB par paire correcte : {mean_extra_correct:.2f}\")\n",
    "\n",
    "print(\"\\nTop 20 relations KB hors GT :\")\n",
    "for rel, count in kb_predictions_outside_gt.most_common(20):\n",
    "    print(f\"{rel}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
